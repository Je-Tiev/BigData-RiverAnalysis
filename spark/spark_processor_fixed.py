import logging
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, split, regexp_extract, regexp_replace, trim, when, expr, lower, explode, array, lit, size, from_json, to_timestamp, avg, min, max
from pyspark.sql.types import *
import os
import sys

# C·∫•u h√¨nh Logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- 0. SETUP JAVA V√Ä ENVIRONMENT ---
def find_java_home():
    """T√¨m JAVA_HOME t·ª± ƒë·ªông n·∫øu kh√¥ng ƒë∆∞·ª£c set"""
    try:
        # C√°ch 1: Check JAVA_HOME ƒë√£ set ch∆∞a
        if "JAVA_HOME" in os.environ:
            java_home = os.environ["JAVA_HOME"]
            if os.path.exists(os.path.join(java_home, "bin", "java.exe")):
                return java_home
        
        # C√°ch 2: T√¨m Java t·ª´ registry (Windows)
        try:
            result = subprocess.run(
                ['powershell', '-Command', 
                 '(Get-Item "HKLM:\\Software\\JavaSoft\\Java Runtime Environment").Property | '
                 'ForEach-Object { $key = Get-ItemProperty "HKLM:\\Software\\JavaSoft\\Java Runtime Environment\\$_"; '
                 'if ($key.JavaHome) { return $key.JavaHome } }'],
                capture_output=True, text=True, timeout=5
            )
            if result.stdout:
                java_home = result.stdout.strip()
                if os.path.exists(java_home):
                    return java_home
        except:
            pass
        
        # C√°ch 3: T√¨m t·ª´ Java command
        try:
            result = subprocess.run(
                ['java', '-version'], 
                capture_output=True, text=True, timeout=5
            )
            if result.returncode == 0:
                # T√¨m java.exe path
                result = subprocess.run(
                    ['where', 'java'], 
                    capture_output=True, text=True, timeout=5
                )
                java_path = result.stdout.strip().split('\n')[0]
                if java_path:
                    # Tr√≠ch JAVA_HOME t·ª´ bin/java.exe
                    java_home = os.path.dirname(os.path.dirname(java_path))
                    return java_home
        except:
            pass
        
        logger.warning("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y JAVA_HOME t·ª± ƒë·ªông")
        return None
    except Exception as e:
        logger.error(f"‚ùå L·ªói t√¨m Java: {str(e)}")
        return None

# Set JAVA_HOME
java_home = find_java_home()
if java_home:
    os.environ["JAVA_HOME"] = java_home
    logger.info(f"‚úÖ JAVA_HOME ƒë·∫∑t th√†nh: {java_home}")
else:
    logger.error("‚ùå Kh√¥ng t√¨m th·∫•y Java. Vui l√≤ng c√†i JDK 8 ho·∫∑c cao h∆°n!")
    logger.error("   T·∫£i t·ª´: https://www.oracle.com/java/technologies/downloads/")
    sys.exit(1)

# X√≥a HADOOP_HOME ƒë·ªÉ tr√°nh conflict
if "HADOOP_HOME" in os.environ:
    del os.environ["HADOOP_HOME"]

# --- 1. KH·ªûI T·∫†O SPARK V·ªöI MONGODB CONNECTOR V√Ä ERROR HANDLING ---
try:
    spark = SparkSession.builder \
        .appName("RiverQualityRealtimeProcessor") \
        .master("local[*]") \
        .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.mongodb.spark:mongo-spark-connector_2.12:10.2.1") \
        .config("spark.mongodb.write.connection.uri", "mongodb://localhost:27017/river_monitoring.sensor_data") \
        .config("spark.mongodb.read.connection.uri", "mongodb://localhost:27017/river_monitoring.sensor_data") \
        .config("spark.sql.streaming.checkpointLocation", "./checkpoints") \
        .config("spark.local.dir", "./spark-tmp") \
        .config("spark.driver.memory", "2g") \
        .config("spark.executor.memory", "2g") \
        .config("spark.sql.shuffle.partitions", "4") \
        .getOrCreate()

    spark.sparkContext.setLogLevel("WARN")
    logger.info("‚úÖ Spark session kh·ªüi t·∫°o th√†nh c√¥ng")
    
except Exception as e:
    logger.error(f"‚ùå L·ªói kh·ªüi t·∫°o Spark session: {str(e)}")
    raise

# --- 2. ƒê·ªäNH NGHƒ®A SCHEMA ---
schema = StructType([
    StructField("FullDate", StringType(), True),
    StructField("WaterbodyName", StringType(), True),
    StructField("Temperature", FloatType(), True),
    StructField("pH", FloatType(), True),
    StructField("Dissolved Oxygen", FloatType(), True),
    StructField("Conductivity @25¬∞C", FloatType(), True),
    StructField("Ammonia-Total (as N)", FloatType(), True),
    StructField("BOD - 5 days (Total)", FloatType(), True),
    StructField("Chloride", FloatType(), True),
    StructField("Total Hardness (as CaCO3)", FloatType(), True),
    StructField("CCME_WQI", StringType(), True),
    StructField("CCME_Values", FloatType(), True)
])

# --- 3. ƒê·ªåC T·ª™ KAFKA ---
try:
    logger.info("üì° ƒêang k·∫øt n·ªëi t·ªõi Kafka...")
    raw_stream = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "localhost:29092") \
        .option("subscribe", "river_sensors") \
        .option("startingOffsets", "latest") \
        .load()
    
    logger.info("‚úÖ Kafka connection successful")
    
except Exception as e:
    logger.error(f"‚ùå Kafka connection failed: {str(e)}")
    raise

# --- 4. X·ª¨ L√ù D·ªÆ LI·ªÜU (TRANSFORMATION) - PARSE JSON & DATA CLEANING ---
try:
    # Parse JSON & ƒê·ªïi t√™n c·ªôt
    parsed_df = raw_stream.selectExpr("CAST(value AS STRING)") \
        .select(from_json(col("value"), schema).alias("data")) \
        .select(
            col("data.FullDate").alias("timestamp"),
            col("data.WaterbodyName").alias("location"),
            col("data.Temperature").alias("temp"),
            col("data.pH").alias("ph"),
            col("data.`Dissolved Oxygen`").alias("do_mgL"),
            col("data.`Conductivity @25¬∞C`").alias("conductivity"),
            col("data.`Ammonia-Total (as N)`").alias("ammonia"),
            col("data.`BOD - 5 days (Total)`").alias("bod"),
            col("data.`Chloride`").alias("chloride"),
            col("data.`Total Hardness (as CaCO3)`").alias("hardness"),
            col("data.CCME_WQI").alias("wqi_category_ref"),
            col("data.CCME_Values").alias("wqi_score_ref")
        )
    
    # --- 5. DATA ENRICHMENT & STANDARDIZATION (tham kh·∫£o t·ª´ data_processing_final.py) ---
    
    # Chu·∫©n h√≥a location (gi·ªëng nh∆∞ chu·∫©n h√≥a city trong final.py)
    enriched_df = parsed_df.withColumn(
        "location",
        trim(when(col("location").isNull() | (col("location") == ""), "Unknown")
             .otherwise(col("location")))
    )
    
    # Ph√¢n lo·∫°i ch·∫•t l∆∞·ª£ng n∆∞·ªõc d·ª±a tr√™n WQI
    enriched_df = enriched_df.withColumn(
        "wqi_category",
        when(col("wqi_score_ref") >= 90, "Excellent")
        .when((col("wqi_score_ref") >= 80) & (col("wqi_score_ref") < 90), "Good")
        .when((col("wqi_score_ref") >= 60) & (col("wqi_score_ref") < 80), "Fair")
        .when((col("wqi_score_ref") >= 40) & (col("wqi_score_ref") < 60), "Poor")
        .when(col("wqi_score_ref") < 40, "Very Poor")
        .otherwise(col("wqi_category_ref"))
    )
    
    # ƒê√°nh gi√° r·ªßi ro chi ti·∫øt (Data Enrichment)
    enriched_df = enriched_df.withColumn(
        "quality_assessment",
        when(
            (col("ph").between(6.5, 8.5)) & 
            (col("do_mgL") >= 5.0) & 
            (col("ammonia") < 0.5) &
            (col("bod") < 3.0), 
            "SAFE"
        ).when(
            (col("ph").between(6.0, 9.0)) & 
            (col("do_mgL") >= 3.0) & 
            (col("ammonia") < 2.0),
            "ACCEPTABLE"
        ).otherwise("WARNING")
    )
    
    # T·∫°o c·∫£nh b√°o chi ti·∫øt (th√™m c√°c lo·∫°i c·∫£nh b√°o t·ª´ final.py)
    enriched_df = enriched_df.withColumn(
        "alert_type",
        when(col("ph") < 4.0, "ACID_HIGH_DANGER")
        .when(col("ph") > 9.0, "ALKALI_HIGH_DANGER")
        .when(col("do_mgL") < 2.0, "FISH_KILL_RISK")
        .when(col("do_mgL") < 5.0, "LOW_DISSOLVED_OXYGEN")
        .when(col("ammonia") > 2.0, "TOXIC_AMMONIA")
        .when(col("bod") > 5.0, "HIGH_ORGANIC_POLLUTION")
        .when(col("wqi_score_ref") < 40, "CRITICAL_WATER_QUALITY")
        .otherwise(None)
    )
    
    # Th√™m m·ª©c ƒë·ªô c·∫£nh b√°o
    enriched_df = enriched_df.withColumn(
        "alert_severity",
        when(col("alert_type").isNull(), "NONE")
        .when(col("alert_type").isin("ACID_HIGH_DANGER", "ALKALI_HIGH_DANGER", "FISH_KILL_RISK", "TOXIC_AMMONIA"), "CRITICAL")
        .when(col("alert_type").isin("LOW_DISSOLVED_OXYGEN", "HIGH_ORGANIC_POLLUTION"), "WARNING")
        .when(col("alert_type") == "CRITICAL_WATER_QUALITY", "CRITICAL")
        .otherwise("INFO")
    )
    
    logger.info("‚úÖ Data enrichment completed")
    
except Exception as e:
    logger.error(f"‚ùå Data transformation failed: {str(e)}")
    raise

# --- 6. AGGREGATION & BATCH STATISTICS (tham kh·∫£o batch processing t·ª´ final.py) ---
try:
    # T·∫°o c√°c dataframe ch·ª©a th·ªëng k√™ theo location
    location_stats = enriched_df.groupBy("location").agg(
        count("*").alias("total_readings"),
        avg("temp").alias("avg_temperature"),
        avg("ph").alias("avg_ph"),
        avg("do_mgL").alias("avg_dissolved_oxygen"),
        avg("ammonia").alias("avg_ammonia"),
        avg("bod").alias("avg_bod"),
        min("wqi_score_ref").alias("min_wqi"),
        max("wqi_score_ref").alias("max_wqi")
    )
    
    # Th·ªëng k√™ theo ch·∫•t l∆∞·ª£ng n∆∞·ªõc
    quality_distribution = enriched_df.groupBy("wqi_category").agg(
        count("*").alias("count")
    ).orderBy(col("count").desc())
    
    # Th·ªëng k√™ c·∫£nh b√°o
    alert_distribution = enriched_df.filter(col("alert_type").isNotNull()).groupBy("alert_type", "alert_severity").agg(
        count("*").alias("alert_count")
    ).orderBy(col("alert_count").desc())
    
    logger.info("‚úÖ Statistical calculations completed")
    
except Exception as e:
    logger.error(f"‚ùå Aggregation failed: {str(e)}")
    raise

# --- 7. WRITE STREAMS TO MONGODB & CONSOLE ---
try:
    # L∆∞u t·∫•t c·∫£ d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω v√†o MongoDB
    query_mongo_raw = enriched_df.writeStream \
        .format("mongodb") \
        .option("checkpointLocation", "./checkpoints/mongo_raw_data") \
        .option("forceDeleteTempCheckpointLocation", "true") \
        .outputMode("append") \
        .trigger(processingTime="10 seconds") \
        .start()
    
    logger.info("‚úÖ MongoDB stream for raw data started")
    
    # L∆∞u th·ªëng k√™ theo location
    query_mongo_stats = location_stats.writeStream \
        .format("mongodb") \
        .option("checkpointLocation", "./checkpoints/mongo_location_stats") \
        .option("forceDeleteTempCheckpointLocation", "true") \
        .outputMode("update") \
        .trigger(processingTime="30 seconds") \
        .start()
    
    logger.info("‚úÖ MongoDB stream for location statistics started")
    
    # L∆∞u quality distribution
    query_mongo_quality = quality_distribution.writeStream \
        .format("mongodb") \
        .option("checkpointLocation", "./checkpoints/mongo_quality_dist") \
        .option("forceDeleteTempCheckpointLocation", "true") \
        .outputMode("update") \
        .trigger(processingTime="30 seconds") \
        .start()
    
    logger.info("‚úÖ MongoDB stream for quality distribution started")
    
    # L∆∞u alert distribution
    query_mongo_alerts = alert_distribution.writeStream \
        .format("mongodb") \
        .option("checkpointLocation", "./checkpoints/mongo_alert_dist") \
        .option("forceDeleteTempCheckpointLocation", "true") \
        .outputMode("update") \
        .trigger(processingTime="30 seconds") \
        .start()
    
    logger.info("‚úÖ MongoDB stream for alert distribution started")
    
except Exception as e:
    logger.error(f"‚ùå MongoDB stream failed: {str(e)}")
    raise

# --- 8. CONSOLE OUTPUT FOR DEBUGGING ---
try:
    # Hi·ªÉn th·ªã c√°c d√≤ng c√≥ c·∫£nh b√°o CRITICAL
    query_console_critical = enriched_df.filter(col("alert_severity") == "CRITICAL") \
        .writeStream \
        .outputMode("append") \
        .format("console") \
        .option("truncate", False) \
        .option("numRows", 20) \
        .start()
    
    logger.info("‚úÖ Console stream for critical alerts started")
    
    # Hi·ªÉn th·ªã t·∫•t c·∫£ c·∫£nh b√°o
    query_console_alerts = enriched_df.filter(col("alert_type").isNotNull()) \
        .writeStream \
        .outputMode("append") \
        .format("console") \
        .option("truncate", False) \
        .option("numRows", 20) \
        .start()
    
    logger.info("‚úÖ Console stream for all alerts started")
    
except Exception as e:
    logger.error(f"‚ùå Console stream failed: {str(e)}")
    raise

# --- 9. STREAM MONITORING & GRACEFUL SHUTDOWN ---
try:
    logger.info("üöÄ H·ªá th·ªëng ƒëang ch·∫°y: Kafka -> Spark Streaming -> MongoDB!")
    logger.info("üìä D·ªØ li·ªáu ƒë∆∞·ª£c x·ª≠ l√Ω v√† ph√¢n t√≠ch theo th·ªùi gian th·ª±c...")
    
    spark.streams.awaitAnyTermination()
    
except Exception as e:
    logger.error(f"‚ùå Stream terminated with error: {str(e)}")
    
finally:
    logger.info("üõë Stopping all streams...")
    spark.streams.stop()
    logger.info("‚úÖ All streams stopped gracefully")
    spark.stop()
