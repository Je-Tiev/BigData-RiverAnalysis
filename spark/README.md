# River Quality Monitoring System - Spark Processing Guide

## ğŸ“‹ Má»¥c lá»¥c
- [Tá»•ng quan](#tá»•ng-quan)
- [YÃªu cáº§u há»‡ thá»‘ng](#yÃªu-cáº§u-há»‡-thá»‘ng)
- [Kiáº¿n trÃºc há»‡ thá»‘ng](#kiáº¿n-trÃºc-há»‡-thá»‘ng)
- [Option 1: Local Mode (ÄÆ¡n giáº£n - Development)](#option-1-local-mode-Ä‘Æ¡n-giáº£n---development)
- [Option 2: Cluster Mode (Production - Thá»±c táº¿)](#option-2-cluster-mode-production---thá»±c-táº¿)
- [So sÃ¡nh Local vs Cluster](#so-sÃ¡nh-local-vs-cluster)
- [Xá»­ lÃ½ lá»—i thÆ°á»ng gáº·p](#xá»­-lÃ½-lá»—i-thÆ°á»ng-gáº·p)
- [Monitoring & Debugging](#monitoring--debugging)

---

## ğŸ¯ Tá»•ng quan

Há»‡ thá»‘ng xá»­ lÃ½ dá»¯ liá»‡u cháº¥t lÆ°á»£ng nÆ°á»›c sÃ´ng theo thá»i gian thá»±c sá»­ dá»¥ng:
- **Apache Kafka**: Message streaming
- **Apache Spark**: Stream processing
- **MongoDB**: Data storage
- **Python**: Application logic

Há»‡ thá»‘ng há»— trá»£ **2 cháº¿ Ä‘á»™ cháº¡y**:
1. **Local Mode**: ÄÆ¡n giáº£n, phÃ¹ há»£p cho development/testing
2. **Cluster Mode**: Production-ready, cÃ³ thá»ƒ scale

---

## ğŸ’» YÃªu cáº§u há»‡ thá»‘ng

### Pháº§n má»m báº¯t buá»™c:
- âœ… **Docker Desktop** (â‰¥ 20.x) & **Docker Compose** (â‰¥ 2.x)
- âœ… **Java JDK** (8, 11, hoáº·c 17)
  - Download: [Adoptium OpenJDK](https://adoptium.net/)
  - Verify: `java -version`
- âœ… **Python** (â‰¥ 3.8)
  - Download: [Python.org](https://www.python.org/downloads/)
  - Verify: `python --version`

### Python packages:
```bash
pip install pyspark==3.5.0
pip install kafka-python
pip install pymongo
```

### Hardware tá»‘i thiá»ƒu:
| Component | Local Mode | Cluster Mode |
|-----------|------------|--------------|
| RAM | 4 GB | 8 GB |
| CPU | 2 cores | 4 cores |
| Disk | 10 GB | 20 GB |

---

## ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng

### Local Mode Architecture:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Docker Containers                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Kafka   â”‚  â”‚ MongoDB  â”‚  â”‚ Kafka UI â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚            â”‚            â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
    â”‚  HOST MACHINE                      â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚  Spark (Local Mode)          â”‚  â”‚
    â”‚  â”‚  spark_processor_local.py    â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Cluster Mode Architecture:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Docker Containers                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚  Kafka   â”‚  â”‚ MongoDB  â”‚  â”‚  Spark Master       â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  (Cluster Manager)  â”‚â”‚
â”‚                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                          â”‚           â”‚
â”‚                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚                               â”‚  Spark Worker(s)    â”‚â”‚
â”‚                               â”‚  (Task Executors)   â”‚â”‚
â”‚                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚  HOST MACHINE          â”‚
                              â”‚  spark_processor_      â”‚
                              â”‚  cluster.py            â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸŸ¢ Option 1: Local Mode (ÄÆ¡n giáº£n - Development)

### âœ… Khi nÃ o sá»­ dá»¥ng Local Mode?
- ğŸ“ Há»c táº­p, nghiÃªn cá»©u
- ğŸ”§ Development vÃ  testing
- ğŸ’» Cháº¡y trÃªn laptop cÃ¡ nhÃ¢n
- ğŸ“Š Xá»­ lÃ½ dá»¯ liá»‡u nhá» (< 10GB/day)

### ğŸ“ Cáº¥u trÃºc thÆ° má»¥c:
```
river-monitoring/
â”œâ”€â”€ docker-compose-local.yml          # â† Docker config (khÃ´ng cÃ³ Spark containers)
â”œâ”€â”€ spark_processor_local_mode.py     # â† Code Python
â”œâ”€â”€ data/
â”‚   â””â”€â”€ river_data.csv
â”œâ”€â”€ checkpoints/                      # â† Spark checkpoints (auto-created)
â”œâ”€â”€ spark-tmp/                        # â† Spark temp dir (auto-created)
â””â”€â”€ README.md
```

### ğŸš€ BÆ°á»›c 1: Chuáº©n bá»‹ Docker Compose

Sá»­ dá»¥ng file `docker-compose-local.yml` (KHÃ”NG bao gá»“m spark-master/spark-worker):

```yaml
# docker-compose-local.yml
version: '3.8'

services:
  kafka-broker:
    image: apache/kafka:4.0.1
    container_name: kafka-broker
    ports:
      - "29092:29092"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093,PLAINTEXT_HOST://0.0.0.0:29092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-broker:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka-broker:9093
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_NUM_PARTITIONS: 3
      CLUSTER_ID: kafka-cluster-1
    networks:
      - bigdata-network

  kafka-controller:
    image: apache/kafka:4.0.1
    container_name: kafka-controller
    environment:
      KAFKA_NODE_ID: 2
      KAFKA_PROCESS_ROLES: controller
      KAFKA_LISTENERS: CONTROLLER://0.0.0.0:9093
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka-broker:9093,2@kafka-controller:9093
      CLUSTER_ID: kafka-cluster-1
    networks:
      - bigdata-network

  mongodb:
    image: mongo:5.0
    container_name: mongodb
    ports:
      - "27017:27017"
    environment:
      MONGO_INITDB_DATABASE: river_monitoring
    volumes:
      - mongodb_data:/data/db
    networks:
      - bigdata-network

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka-broker:29092
      DYNAMIC_CONFIG_ENABLED: 'true'
    depends_on:
      - kafka-broker
    networks:
      - bigdata-network

networks:
  bigdata-network:
    driver: bridge

volumes:
  mongodb_data:
```

### ğŸš€ BÆ°á»›c 2: Khá»Ÿi Ä‘á»™ng Docker containers

```bash
# Start containers
docker-compose -f docker-compose-local.yml up -d

# Verify containers Ä‘ang cháº¡y
docker ps

# Káº¿t quáº£ mong Ä‘á»£i:
# âœ… kafka-broker
# âœ… kafka-controller  
# âœ… mongodb
# âœ… kafka-ui
# âŒ spark-master (KHÃ”NG CÃ“)
# âŒ spark-worker (KHÃ”NG CÃ“)

# Check logs náº¿u cáº§n
docker-compose -f docker-compose-local.yml logs -f
```

### ğŸš€ BÆ°á»›c 3: Kiá»ƒm tra services

```bash
# Test Kafka
docker exec -it kafka-broker kafka-topics.sh \
  --bootstrap-server localhost:9092 --list

# Test MongoDB
docker exec -it mongodb mongosh --eval "db.adminCommand('ping')"

# Hoáº·c truy cáº­p UIs:
# - Kafka UI: http://localhost:8080
# - MongoDB: mongosh hoáº·c MongoDB Compass (localhost:27017)
```

### ğŸš€ BÆ°á»›c 4: Cháº¡y Spark code (trÃªn HOST)

```bash
# CHáº Y TRÃŠN HOST MACHINE (khÃ´ng trong Docker)
python spark_processor_local_mode.py

# Output mong Ä‘á»£i:
# âœ… JAVA_HOME: C:\Program Files\Java\jdk-11
# âœ… SPARK LOCAL MODE ACTIVATED
# ğŸ¯ Master: local[*]
# ğŸ“Š Parallelism: 8
# ğŸ“¡ Kafka connection successful
# ğŸ’¾ MongoDB streams started
# ğŸš€ Há»† THá»NG ÄANG CHáº Y - LOCAL MODE
```

### ğŸ›‘ BÆ°á»›c 5: Dá»«ng há»‡ thá»‘ng

```bash
# Dá»«ng Spark code (Ctrl+C trong terminal Ä‘ang cháº¡y Python)

# Dá»«ng Docker containers
docker-compose -f docker-compose-local.yml down

# XÃ³a volumes náº¿u muá»‘n reset data
docker-compose -f docker-compose-local.yml down -v
```

---

## ğŸ”µ Option 2: Cluster Mode (Production - Thá»±c táº¿)

### âœ… Khi nÃ o sá»­ dá»¥ng Cluster Mode?
- ğŸ¢ MÃ´i trÆ°á»ng production
- ğŸ“ˆ Cáº§n scale (xá»­ lÃ½ nhiá»u data)
- ğŸ“ Há»c kiáº¿n trÃºc distributed system
- ğŸ’¼ Demo cho portfolio/interview

### ğŸ“ Cáº¥u trÃºc thÆ° má»¥c:
```
river-monitoring/
â”œâ”€â”€ docker-compose-cluster.yml        # â† Docker config (cÃ³ Spark containers)
â”œâ”€â”€ spark_processor_cluster.py        # â† Code Python
â”œâ”€â”€ data/
â”œâ”€â”€ checkpoints/
â””â”€â”€ README.md
```

### ğŸš€ BÆ°á»›c 1: Chuáº©n bá»‹ Docker Compose

Sá»­ dá»¥ng file `docker-compose-cluster.yml` (BAO Gá»’M spark-master/spark-worker):

```yaml
# docker-compose-cluster.yml
version: '3.8'

services:
  # === KAFKA SERVICES ===
  kafka-broker:
    image: apache/kafka:4.0.1
    container_name: kafka-broker
    ports:
      - "29092:29092"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093,PLAINTEXT_HOST://0.0.0.0:29092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-broker:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka-broker:9093
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_NUM_PARTITIONS: 3
      CLUSTER_ID: kafka-cluster-1
    networks:
      - bigdata-network

  kafka-controller:
    image: apache/kafka:4.0.1
    container_name: kafka-controller
    environment:
      KAFKA_NODE_ID: 2
      KAFKA_PROCESS_ROLES: controller
      KAFKA_LISTENERS: CONTROLLER://0.0.0.0:9093
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka-broker:9093,2@kafka-controller:9093
      CLUSTER_ID: kafka-cluster-1
    networks:
      - bigdata-network

  # === MONGODB ===
  mongodb:
    image: mongo:5.0
    container_name: mongodb
    ports:
      - "27017:27017"
    environment:
      MONGO_INITDB_DATABASE: river_monitoring
    volumes:
      - mongodb_data:/data/db
    networks:
      - bigdata-network

  # === SPARK CLUSTER ===
  spark-master:
    image: bitnami/spark:3.5.7
    container_name: spark-master
    ports:
      - "7077:7077"    # Cluster port
      - "8081:8080"    # Web UI
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    networks:
      - bigdata-network

  spark-worker:
    image: bitnami/spark:3.5.7
    container_name: spark-worker
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    networks:
      - bigdata-network

  # === KAFKA UI ===
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka-broker:29092
      DYNAMIC_CONFIG_ENABLED: 'true'
    depends_on:
      - kafka-broker
    networks:
      - bigdata-network

networks:
  bigdata-network:
    driver: bridge

volumes:
  mongodb_data:
```

### ğŸš€ BÆ°á»›c 2: Khá»Ÿi Ä‘á»™ng Docker containers

```bash
# Start ALL containers (bao gá»“m Spark cluster)
docker-compose -f docker-compose-cluster.yml up -d

# Verify containers Ä‘ang cháº¡y
docker ps

# Káº¿t quáº£ mong Ä‘á»£i:
# âœ… kafka-broker
# âœ… kafka-controller
# âœ… mongodb
# âœ… kafka-ui
# âœ… spark-master    â† Má»šI
# âœ… spark-worker    â† Má»šI

# Chá» 10-15 giÃ¢y Ä‘á»ƒ Spark cluster khá»Ÿi Ä‘á»™ng
```

### ğŸš€ BÆ°á»›c 3: Verify Spark cluster

```bash
# Check Spark Master Web UI
# Má»Ÿ browser: http://localhost:8081

# Pháº£i tháº¥y:
# âœ… Status: ALIVE
# âœ… Workers: 1
# âœ… Cores: 2 Total, 2 Available
# âœ… Memory: 2.0 GB Total, 2.0 GB Available

# Check logs
docker logs spark-master
docker logs spark-worker
```

### ğŸš€ BÆ°á»›c 4: Cháº¡y Spark code (cluster mode)

```bash
# CHáº Y TRÃŠN HOST MACHINE
python spark_processor_cluster.py

# Output mong Ä‘á»£i:
# âœ… JAVA_HOME: C:\Program Files\Java\jdk-11
# ğŸ”§ Äang khá»Ÿi táº¡o Spark Cluster Mode...
# âœ… SPARK CLUSTER MODE ACTIVATED
# ğŸ¯ Master URL: spark://spark-master:7077
# ğŸ“Š Default Parallelism: 2
# ğŸ’¾ Executor Memory: 1g
# ğŸ”¢ Executor Cores: 2
# ğŸ“¡ Kafka connection successful
# ğŸš€ Há»† THá»NG ÄANG CHáº Y - CLUSTER MODE
```

### ğŸš€ BÆ°á»›c 5: Monitor cluster

```bash
# Spark Master UI
http://localhost:8081
# Xem: Workers, Running Applications, Completed Applications

# Spark Application UI (khi job Ä‘ang cháº¡y)
http://localhost:4040
# Xem: Jobs, Stages, Storage, Environment

# Kafka UI
http://localhost:8080
# Xem: Topics, Messages, Consumer Groups
```

### ğŸ›‘ BÆ°á»›c 6: Dá»«ng há»‡ thá»‘ng

```bash
# Dá»«ng Spark code (Ctrl+C)

# Dá»«ng Docker containers
docker-compose -f docker-compose-cluster.yml down

# Clean up volumes
docker-compose -f docker-compose-cluster.yml down -v
```

---

## âš–ï¸ So sÃ¡nh Local vs Cluster

| TiÃªu chÃ­ | Local Mode | Cluster Mode |
|----------|------------|--------------|
| **Äá»™ phá»©c táº¡p** | ğŸŸ¢ ÄÆ¡n giáº£n | ğŸŸ¡ Trung bÃ¬nh |
| **Setup time** | ğŸŸ¢ 5 phÃºt | ğŸŸ¡ 10-15 phÃºt |
| **Docker containers** | 4-5 | 7-8 |
| **RAM cáº§n** | 4 GB | 8 GB |
| **CPU cáº§n** | 2 cores | 4 cores |
| **Spark Master** | `.master("local[*]")` | `.master("spark://spark-master:7077")` |
| **Kafka address** | `localhost:29092` | `kafka-broker:29092` (trong Docker)<br>`localhost:29092` (tá»« host) |
| **MongoDB address** | `localhost:27017` | `mongodb:27017` (trong Docker)<br>`localhost:27017` (tá»« host) |
| **Scalability** | âŒ KhÃ´ng scale | âœ… ThÃªm workers dá»… dÃ ng |
| **Production-ready** | âŒ KhÃ´ng | âœ… CÃ³ |
| **Learning value** | ğŸŸ¡ CÆ¡ báº£n | ğŸŸ¢ Cao (distributed system) |
| **Debug** | ğŸŸ¢ Dá»… | ğŸŸ¡ Phá»©c táº¡p hÆ¡n |
| **Use case** | Dev, Testing, Learning | Production, Portfolio, Scale |

### ğŸ“Š Khi nÃ o dÃ¹ng gÃ¬?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Náº¿u báº¡n...                          â†’ Chá»n         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Äang há»c Spark láº§n Ä‘áº§u              â†’ Local Mode   â”‚
â”‚  Chá»‰ muá»‘n test code nhanh            â†’ Local Mode   â”‚
â”‚  Laptop yáº¿u (4GB RAM)                â†’ Local Mode   â”‚
â”‚  Muá»‘n há»c kiáº¿n trÃºc thá»±c táº¿          â†’ Cluster Mode â”‚
â”‚  Cáº§n cho portfolio/CV                â†’ Cluster Mode â”‚
â”‚  Chuáº©n bá»‹ deploy production          â†’ Cluster Mode â”‚
â”‚  Data lá»›n (>10GB/day)                â†’ Cluster Mode â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš ï¸ LÆ°u Ã½ quan trá»ng khi cháº¡y cáº£ 2 modes

### ğŸš¨ **NGUYÃŠN Táº®C: KHÃ”NG cháº¡y Ä‘á»“ng thá»i 2 modes**

#### âŒ **KHÃ”NG ÄÆ¯á»¢C LÃ€M:**
```bash
# Terminal 1
python spark_processor_local_mode.py      # Äang cháº¡y

# Terminal 2
python spark_processor_cluster.py         # âŒ XUNG Äá»˜T!
```

#### âœ… **LÃ€M ÄÃšNG:**
```bash
# Cháº¡y Local Mode
python spark_processor_local_mode.py
# Ctrl+C Ä‘á»ƒ dá»«ng

# SAU ÄÃ“ má»›i cháº¡y Cluster Mode
python spark_processor_cluster.py
```

### ğŸ”´ **LÃ½ do KHÃ”NG Ä‘Æ°á»£c cháº¡y Ä‘á»“ng thá»i:**

1. **Xung Ä‘á»™t port Spark UI**
   - Cáº£ 2 Ä‘á»u dÃ¹ng port `4040` cho Application UI
   - Process thá»© 2 sáº½ bá»‹ lá»—i hoáº·c dÃ¹ng port khÃ¡c (4041, 4042...)

2. **Xung Ä‘á»™t Kafka Consumer Group**
   - Cáº£ 2 Ä‘á»u Ä‘á»c tá»« cÃ¹ng topic `river_sensors`
   - Kafka sáº½ rebalance partitions â†’ dá»¯ liá»‡u bá»‹ phÃ¢n tÃ¡n

3. **Xung Ä‘á»™t MongoDB writes**
   - Cáº£ 2 Ä‘á»u ghi vÃ o cÃ¹ng collection
   - CÃ³ thá»ƒ gÃ¢y duplicate data hoáº·c checkpoint conflicts

4. **Xung Ä‘á»™t Checkpoint directory**
   - Cáº£ 2 Ä‘á»u dÃ¹ng `./checkpoints/`
   - Spark sáº½ bÃ¡o lá»—i checkpoint corruption

### âœ… **CÃ¡ch chuyá»ƒn Ä‘á»•i giá»¯a 2 modes:**

#### **Tá»« Local â†’ Cluster:**
```bash
# 1. Dá»«ng Local Mode (Ctrl+C)

# 2. Stop Docker local
docker-compose -f docker-compose-local.yml down

# 3. XÃ“A checkpoints cÅ© (QUAN TRá»ŒNG!)
rm -rf ./checkpoints/*
# Windows: rmdir /s /q checkpoints

# 4. Start Cluster
docker-compose -f docker-compose-cluster.yml up -d

# 5. Chá» 10 giÃ¢y Ä‘á»ƒ Spark cluster ready

# 6. Cháº¡y Cluster Mode
python spark_processor_cluster.py
```

#### **Tá»« Cluster â†’ Local:**
```bash
# 1. Dá»«ng Cluster Mode (Ctrl+C)

# 2. Stop Docker cluster
docker-compose -f docker-compose-cluster.yml down

# 3. XÃ“A checkpoints cÅ© (QUAN TRá»ŒNG!)
rm -rf ./checkpoints/*

# 4. Start Local
docker-compose -f docker-compose-local.yml up -d

# 5. Cháº¡y Local Mode
python spark_processor_local_mode.py
```

### ğŸ—‘ï¸ **Script tá»± Ä‘á»™ng clean checkpoints:**

Táº¡o file `reset.sh` (Linux/Mac) hoáº·c `reset.bat` (Windows):

**Linux/Mac (reset.sh):**
```bash
#!/bin/bash
echo "ğŸ§¹ Cleaning checkpoints..."
rm -rf ./checkpoints/*
rm -rf ./spark-tmp/*
echo "âœ… Clean completed!"
```

**Windows (reset.bat):**
```bat
@echo off
echo ğŸ§¹ Cleaning checkpoints...
rmdir /s /q checkpoints 2>nul
rmdir /s /q spark-tmp 2>nul
mkdir checkpoints
mkdir spark-tmp
echo âœ… Clean completed!
```

**Sá»­ dá»¥ng:**
```bash
# Linux/Mac
chmod +x reset.sh
./reset.sh

# Windows
reset.bat
```

### ğŸ“‹ **Checklist trÆ°á»›c khi chuyá»ƒn mode:**

- [ ] Dá»«ng Python process hiá»‡n táº¡i (Ctrl+C)
- [ ] Stop Docker containers (`docker-compose down`)
- [ ] XÃ³a checkpoints (`rm -rf ./checkpoints/*`)
- [ ] XÃ³a spark-tmp (`rm -rf ./spark-tmp/*`)
- [ ] Start Docker vá»›i config má»›i
- [ ] Verify containers Ä‘ang cháº¡y (`docker ps`)
- [ ] Cháº¡y Python script má»›i

---

## ğŸ› Xá»­ lÃ½ lá»—i thÆ°á»ng gáº·p

### âŒ Lá»—i 1: Java not found

```bash
ERROR: JAVA_HOME is not set and no 'java' command could be found

# FIX:
# 1. CÃ i Java JDK 8/11/17
# Download: https://adoptium.net/

# 2. Set JAVA_HOME
# Windows:
set JAVA_HOME=C:\Program Files\Java\jdk-11
set PATH=%JAVA_HOME%\bin;%PATH%

# Linux/Mac:
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk
export PATH=$JAVA_HOME/bin:$PATH

# 3. Verify
java -version
```

### âŒ Lá»—i 2: Kafka connection refused

```bash
ERROR: Connection to node -1 (localhost:29092) could not be established

# FIX:
# 1. Check Kafka Ä‘ang cháº¡y
docker ps | grep kafka

# 2. Check port
docker port kafka-broker

# 3. Test connection
telnet localhost 29092
# Hoáº·c
nc -zv localhost 29092

# 4. Restart Kafka náº¿u cáº§n
docker-compose restart kafka-broker
```

### âŒ Lá»—i 3: MongoDB timeout

```bash
ERROR: com.mongodb.MongoTimeoutException: Timed out after 30000 ms

# FIX:
# 1. Check MongoDB Ä‘ang cháº¡y
docker ps | grep mongodb

# 2. Test connection
docker exec -it mongodb mongosh --eval "db.adminCommand('ping')"

# 3. Check port
docker port mongodb

# 4. Restart MongoDB
docker-compose restart mongodb
```

### âŒ Lá»—i 4: Spark Master connection refused (Cluster Mode)

```bash
ERROR: Could not connect to spark-master:7077

# FIX:
# 1. Check Spark Master Ä‘ang cháº¡y
docker ps | grep spark-master

# 2. Check Spark Master UI
# Browser: http://localhost:8081

# 3. Check logs
docker logs spark-master
docker logs spark-worker

# 4. Verify worker connected
# Trong Spark Master UI pháº£i tháº¥y: Workers: 1

# 5. Restart Spark cluster
docker-compose restart spark-master spark-worker
```

### âŒ Lá»—i 5: Port already in use

```bash
ERROR: Bind for 0.0.0.0:4040 failed: port is already allocated

# FIX:
# 1. Find process using port
# Windows:
netstat -ano | findstr :4040
taskkill /PID <process_id> /F

# Linux/Mac:
lsof -i :4040
kill -9 <process_id>

# 2. Hoáº·c dá»«ng Spark process cÅ©
# TÃ¬m vÃ  kill táº¥t cáº£ process Python Ä‘ang cháº¡y Spark
```

### âŒ Lá»—i 6: Checkpoint already exists

```bash
ERROR: Checkpoint directory already exists

# FIX:
# XÃ³a checkpoints cÅ©
rm -rf ./checkpoints/*

# Windows:
rmdir /s /q checkpoints
mkdir checkpoints
```

### âŒ Lá»—i 7: Insufficient memory

```bash
ERROR: Not enough memory to create Java heap

# FIX Local Mode:
# Giáº£m memory config trong code
.config("spark.driver.memory", "1g")  # Tá»« 2g â†’ 1g
.config("spark.executor.memory", "1g")

# FIX Cluster Mode:
# Sá»­a docker-compose-cluster.yml
environment:
  - SPARK_WORKER_MEMORY=1G  # Tá»« 2G â†’ 1G
```

---

## ğŸ“Š Monitoring & Debugging

### ğŸŒ **Web UIs:**

| Service | URL | Má»¥c Ä‘Ã­ch |
|---------|-----|----------|
| Kafka UI | http://localhost:8080 | Monitor Kafka topics, messages |
| Spark Master UI | http://localhost:8081 | Monitor cluster (Cluster Mode only) |
| Spark Application UI | http://localhost:4040 | Monitor running job |
| MongoDB Compass | localhost:27017 | Browse database |

### ğŸ“ **Useful Docker commands:**

```bash
# View all containers
docker ps -a

# View logs
docker logs -f kafka-broker
docker logs -f mongodb
docker logs -f spark-master
docker logs -f spark-worker

# Enter container
docker exec -it kafka-broker bash
docker exec -it mongodb mongosh

# Check resource usage
docker stats

# Restart specific service
docker-compose restart kafka-broker

# View networks
docker network ls
docker network inspect bigdata-network
```

### ğŸ” **Debugging Spark jobs:**

```bash
# Check Spark logs trong console output

# Check Spark UI (http://localhost:4040)
# - Jobs: Xem progress cá»§a tá»«ng job
# - Stages: Chi tiáº¿t tá»«ng stage
# - Storage: Memory/Disk usage
# - Environment: Config Ä‘ang dÃ¹ng
# - Executors: Resource usage

# Check checkpoints
ls -la ./checkpoints/

# Check Spark temp
ls -la ./spark